{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Exercise: Script Mode in SageMaker\n",
    "\n",
    "For this exercise, you will have to train a model in SageMaker in script mode. To complete this exercise you will need to complete all the TODO's in the `script_mode.py` and `scripts/pytorch_cifar.py` python file.\n",
    "\n",
    "In case you get stuck, you can look at the solution by clicking the 'Solution' buttons.\n",
    "\n",
    "**Note**: You cannot run this file in the classroom. You will have to move these files to your SageMaker Studio instance. However, to reduce the amount of credits you use, you can develop and test your file in this workspace before running it in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### `script_mode.py` Tasks\n",
    "1. Include the hyperparameters your script will need over here.\n",
    "2. Create your estimator here. You can use Pytorch or any other framework.\n",
    "3. Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-9f16271f": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION  for script_mode.py",
       "toggleOnText": "SHOW SOLUTION for script_mode.py"
      }
     }
    }
   },
   "source": [
    "``` python\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "hyperparameters = {\"epochs\": \"2\", \"batch-size\": \"32\", \"test-batch-size\": \"100\", \"lr\": \"0.001\"}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"scripts/pytorch_cifar.py\",\n",
    "    base_job_name=\"sagemaker-script-mode\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py36\",\n",
    ")\n",
    "\n",
    "estimator.fit(wait=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### `pytorch_cifar.py` Tasks\n",
    "1. Complete the `__init__` function.\n",
    "2. Complete the `forward` function.\n",
    "3. In `main`\n",
    "    - Add arguments\n",
    "    - Create a transform\n",
    "    - Add the CIFAR10 dataset\n",
    "    - Create data loaders\n",
    "    - Add optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-6a4eef0c": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION for pytorch_cifar.py",
       "toggleOnText": "SHOW SOLUTION for pytorch_cifar.py"
      }
     }
    }
   },
   "source": [
    "```python\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=14,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 14)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", type=float, default=1.0, metavar=\"LR\", help=\"learning rate (default: 1.0)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_kwargs = {\"batch_size\": args.batch_size}\n",
    "    test_kwargs = {\"batch_size\": args.test_batch_size}\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    )\n",
    "    dataset1 = datasets.CIFAR10(\"../data\", train=True, download=True, transform=transform)\n",
    "    dataset2 = datasets.CIFAR10(\"../data\", train=False, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net()\n",
    "\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(model, train_loader, optimizer, epoch)\n",
    "        test(model, test_loader)\n",
    "    \n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Try It Out!\n",
    "- Can you use an estimator of a different framework to train your model\n",
    "- Can you change some of the hyperparameters and retrain your model? How does the performance of the model change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Exercise: Debugger and Profiler\n",
    "\n",
    "In this exercise, we will combine both debugging and profiling and train a model. Here are the tasks for this exercise:\n",
    "- Most of the training script `scirpts/pytorch_cifar_profiling.py` has been completed for you. However you still need to add the debugger hook. Finish all the TODO's in the file to do that.\n",
    "- Finish all the TODO's in this file to train a model using sagemaker debugger and profiler.\n",
    "\n",
    "In case you get stuck, you can look at the solution by clicking the 'Solution' buttons.\n",
    "\n",
    "**Note**: You cannot run this file in the classroom. You will have to move these files to your SageMaker Studio instance. However, to reduce the amount of credits you use, you can develop and test your file in this workspace before running it in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### `model_profiling.ipynb` Tasks\n",
    "1. Add rules you want to create in `rules` list.\n",
    "2. Create the profilier and debugger configurations.\n",
    "3. Create the estimator to train your model.\n",
    "4. Print the names of all the tensors that were tracked.\n",
    "5. Print the number of datapoints for one of those tensors for both train and eval mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-cba991b4": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION for model_profiling.ipynb",
       "toggleOnText": "SHOW SOLUTION for model_profiling.ipynb"
      }
     }
    }
   },
   "source": [
    "```python\n",
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "\n",
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "]\n",
    "\n",
    "from sagemaker.debugger import DebuggerHookConfig, ProfilerConfig, FrameworkProfile\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")\n",
    "debugger_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"train.save_interval\": \"100\", \"eval.save_interval\": \"10\"}\n",
    ")\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"pytorch_cifar_profiling.py\",\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py36\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    profiler_config=profiler_config,\n",
    "    debugger_hook_config=debugger_config,\n",
    "    rules=rules,\n",
    ")\n",
    "\n",
    "estimator.fit(wait=True)\n",
    "\n",
    "import boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f\"Training jobname: {training_job_name}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "\n",
    "trial = create_trial(estimator.latest_job_debugger_artifacts_path())\n",
    "\n",
    "\n",
    "print(trial.tensor_names())\n",
    "print(len(trial.tensor(\"CrossEntropyLoss_output_0\").steps(mode=ModeKeys.TRAIN)))\n",
    "print(len(trial.tensor(\"CrossEntropyLoss_output_0\").steps(mode=ModeKeys.EVAL)))\n",
    "\n",
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "\n",
    "tj = TrainingJob(training_job_name, region)\n",
    "tj.wait_for_sys_profiling_data_to_be_available()\n",
    "\n",
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\n",
    "\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "view_timeline_charts = TimelineCharts(\n",
    "    system_metrics_reader,\n",
    "    framework_metrics_reader=None,\n",
    "    select_dimensions=[\"CPU\", \"GPU\"],\n",
    "    select_events=[\"total\"],\n",
    ")\n",
    "\n",
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")\n",
    "\n",
    "! aws s3 ls {rule_output_path} --recursive\n",
    "! aws s3 cp {rule_output_path} ./ --recursive\n",
    "\n",
    "import os\n",
    "\n",
    "# get the autogenerated folder name of profiler report\n",
    "profiler_report_name = [\n",
    "    rule[\"RuleConfigurationName\"]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"Profiler\" in rule[\"RuleConfigurationName\"]\n",
    "][0]\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.display.HTML(filename=profiler_report_name + \"/profiler-output/profiler-report.html\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-05c28385": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION  for model_profiling.ipynb CLEANED",
       "toggleOnText": "SHOW SOLUTION for model_profiling.ipynb CLEANED"
      }
     }
    }
   },
   "source": [
    "```python\n",
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "from sagemaker.debugger import DebuggerHookConfig, ProfilerConfig, FrameworkProfile\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "]\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")\n",
    "debugger_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"train.save_interval\": \"100\", \"eval.save_interval\": \"10\"}\n",
    ")\n",
    "\n",
    "estimator = PyTorch(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"pytorch_cifar_profiling.py\",\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py36\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    profiler_config=profiler_config,\n",
    "    debugger_hook_config=debugger_config,\n",
    "    rules=rules,\n",
    ")\n",
    "\n",
    "estimator.fit(wait=True)\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f\"Training jobname: {training_job_name}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "trial = create_trial(estimator.latest_job_debugger_artifacts_path())\n",
    "\n",
    "print(trial.tensor_names())\n",
    "print(len(trial.tensor(\"CrossEntropyLoss_output_0\").steps(mode=ModeKeys.TRAIN)))\n",
    "print(len(trial.tensor(\"CrossEntropyLoss_output_0\").steps(mode=ModeKeys.EVAL)))\n",
    "\n",
    "tj = TrainingJob(training_job_name, region)\n",
    "tj.wait_for_sys_profiling_data_to_be_available()\n",
    "\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "view_timeline_charts = TimelineCharts(\n",
    "    system_metrics_reader,\n",
    "    framework_metrics_reader=None,\n",
    "    select_dimensions=[\"CPU\", \"GPU\"],\n",
    "    select_events=[\"total\"],\n",
    ")\n",
    "\n",
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")\n",
    "\n",
    "! aws s3 ls {rule_output_path} --recursive\n",
    "! aws s3 cp {rule_output_path} ./ --recursive\n",
    "\n",
    "# get the autogenerated folder name of profiler report\n",
    "profiler_report_name = [\n",
    "    rule[\"RuleConfigurationName\"]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"Profiler\" in rule[\"RuleConfigurationName\"]\n",
    "][0]\n",
    "\n",
    "IPython.display.HTML(filename=profiler_report_name + \"/profiler-output/profiler-report.html\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### `pytorch_cifar_profiling.py` Tasks\n",
    "\n",
    "1. Create Hook\n",
    "2. Set hook to track the loss\n",
    "3. Set hook to train mode\n",
    "4. Set hook to eval mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-ba282b7b": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION for pythorch_cifar_profiling.py",
       "toggleOnText": "SHOW SOLUTION for pythorch_cifar_profiling.py"
      }
     }
    }
   },
   "source": [
    "``` python\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from smdebug import modes\n",
    "from smdebug.profiler.utils import str2bool\n",
    "from smdebug.pytorch import get_hook\n",
    "\n",
    "def train(args, net, device):\n",
    "    hook = get_hook(create_if_not_exists=True)\n",
    "    batch_size = args.batch_size\n",
    "    epoch = args.epoch\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transform_valid = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    validset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_valid\n",
    "    )\n",
    "    validloader = torch.utils.data.DataLoader(\n",
    "        validset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    loss_optim = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=1.0, momentum=0.9)\n",
    "\n",
    "    epoch_times = []\n",
    "\n",
    "    if hook:\n",
    "        hook.register_loss(loss_optim)\n",
    "    # train the model\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print(\"START TRAINING\")\n",
    "        if hook:\n",
    "            hook.set_mode(modes.TRAIN)\n",
    "        start = time.time()\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        for _, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_optim(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(\"START VALIDATING\")\n",
    "        if hook:\n",
    "            hook.set_mode(modes.EVAL)\n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (inputs, targets) in enumerate(validloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = loss_optim(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        epoch_time = time.time() - start\n",
    "        epoch_times.append(epoch_time)\n",
    "        print(\n",
    "            \"Epoch %d: train loss %.3f, val loss %.3f, in %.1f sec\"\n",
    "            % (i, train_loss, val_loss, epoch_time)\n",
    "        )\n",
    "\n",
    "    # calculate training time after all epoch\n",
    "    p50 = np.percentile(epoch_times, 50)\n",
    "    return p50\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=1)\n",
    "    parser.add_argument(\"--gpu\", type=str2bool, default=True)\n",
    "    parser.add_argument(\"--model\", type=str, default=\"resnet50\")\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    for key, value in vars(opt).items():\n",
    "        print(f\"{key}:{value}\")\n",
    "    # create model\n",
    "    net = models.__dict__[opt.model](pretrained=True)\n",
    "    if opt.gpu == 1:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    # Start the training.\n",
    "    median_time = train(opt, net, device)\n",
    "    print(\"Median training time per Epoch=%.1f sec\" % median_time)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Try It Out!\n",
    "- Can you plot one of the tensors and see how it has evolved throughout the training/eval process\n",
    "- What are some ways to improve the GPU utilization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Exercise: Hyperparameter Tuning in SageMaker\n",
    "\n",
    "For this exercise, you will have to use hyperparameter tuning to tune three different hyperparameters, train the model on the CIFAR10 dataset, deploy it to an endpoint and then query it. The dataset you use should also be present in a S3 bucket.\n",
    "\n",
    "For this exercise, the training script has already been provided for you. You will only need to complete the TODOs in this notebook.\n",
    "Here are your tasks:\n",
    "- Upload CIFAR10 data to an S3 bucket.\n",
    "- Finetune 3 hyperparameters. You can choose one of the hyperparameters that is already added as a command line argument in the training script, or you can add one of your own.\n",
    "- Deploy the best trained model, query it and get the result.\n",
    "\n",
    "In case you get stuck, you can look at the solution by clicking the 'Solution' buttons.\n",
    "\n",
    "**Note**: You cannot run this file in the classroom. You will have to move these files to your SageMaker Studio instance. However, to reduce the amount of credits you use, you can develop and test your file in this workspace before running it in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "#### `hpo_deploy.ipynb` Tasks\n",
    "1. Upload the data to an S3 bucket through sagemaker_session object, boto3 or the AWS CLI.\n",
    "2. Initialise your hyperparameters.\n",
    "3. Create your HyperparameterTuner Object\n",
    "4. Train your model\n",
    "5. Query the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-c8782c19": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION for hpo_deploy.ipynb",
       "toggleOnText": "SHOW SOLUTION for hpo_deploy.ipynb"
      }
     }
    }
   },
   "source": [
    "```python\n",
    "import sagemaker\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cifar\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "local_dir = 'data'\n",
    "CIFAR10.mirrors = [\"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/CIFAR10/\"]\n",
    "CIFAR10(\n",
    "    local_dir,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    )\n",
    ")\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path=\"data\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"cifar.py\",\n",
    "    role=role,\n",
    "    py_version='py36',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.1),\n",
    "    \"batch-size\": CategoricalParameter([32, 64, 128, 256, 512]),\n",
    "    \"epochs\": IntegerParameter(2, 4)\n",
    "}\n",
    "\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type=objective_type,\n",
    ")\n",
    "\n",
    "tuner.fit({\"training\": inputs})\n",
    "\n",
    "predictor = tuner.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n",
    "\n",
    "# Query the Endpoint\n",
    "import gzip \n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "file = 'data/cifar-10-batches-py/data_batch_1'\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data=unpickle(file)\n",
    "data=np.reshape(data[b'data'][0], (3, 32, 32))\n",
    "\n",
    "response = # TODO: Query the endpoint\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-ac952fc2": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION for for hpo_deploy.ipynb CLEANED",
       "toggleOnText": "SHOW SOLUTION for hpo_deploy.ipynb CLEANED"
      }
     }
    }
   },
   "source": [
    "```python\n",
    "import sagemaker\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import gzip \n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cifar\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "local_dir = 'data'\n",
    "CIFAR10.mirrors = [\"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/CIFAR10/\"]\n",
    "CIFAR10(\n",
    "    local_dir,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    )\n",
    ")\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path=\"data\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"cifar.py\",\n",
    "    role=role,\n",
    "    py_version='py36',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.1),\n",
    "    \"batch-size\": CategoricalParameter([32, 64, 128, 256, 512]),\n",
    "    \"epochs\": IntegerParameter(2, 4)\n",
    "}\n",
    "\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type=objective_type,\n",
    ")\n",
    "\n",
    "tuner.fit({\"training\": inputs})\n",
    "\n",
    "predictor = tuner.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n",
    "\n",
    "# Query the Endpoint\n",
    "\n",
    "file = 'data/cifar-10-batches-py/data_batch_1'\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data=unpickle(file)\n",
    "data=np.reshape(data[b'data'][0], (3, 32, 32))\n",
    "\n",
    "response = # TODO: Query the endpoint\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this exercise, remember to delete the prediction endpoint to release the instance associated with it\n",
    "\n",
    "## Try It Out!\n",
    "- Can you use an estimator of a different framework to train a model\n",
    "- Can you figure out the accuracy of your model by querying all the test data to the deployed endpoint?\n",
    "- Can you tune 2 more hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "ulab_nb_type": "guided"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
